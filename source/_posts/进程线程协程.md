---
title: 进程线程协程
author: 张大哥
top: false
cover: false
toc: true
mathjax: false
categories: 后端
tags:
  - python
  - python
  - linux
abbrlink: 63298
date: 2022-10-24 19:25:10
img:
coverImg:
summary:
---

# 进程



### 1.1 什么是进程？

- `1.进程是资源分配的最小单位（ 内存、cpu、网络、io）`

- ```
  2.一个运行起来的程序就是一个进程
  ```

  - 什么是程序（

    ```
    程序是我们存储在硬盘里的代码
    ```

    ）

    - 硬盘（256G）、内存条（8G）
    - 当我们双击图标，打开程序的时候，实际上就是通过I/O操作（读写），硬盘中的代码读取到内存条里

  - 内存条就是我们所指的资源（程序分配了内存资源，就变成了进程）

  - CPU分时

    - CPU比你的手速快多了，分时处理每个线程，但是由于太快然你觉得每个线程都是独占cpu
    - cpu是计算，只有时间片到了，获取cpu，线程真正执行
    - 当你想使用 网络、磁盘等资源的时候，需要cpu的调度

- `3.进程具有独立的内存空间，所以不能直接相互`

### 1.2 进程如何通信

- 同一程序下进程通信
  - 消息队列(父子进程通信)
  - pipe（同一程序下两个进程通信）
  - managers（同一程序下多个进程通信）
  - 共享内存
  - 套接字（不同主机进程之间的通信）
- Java项目和python项目如何通信
  - RabbitMQ、redis等（不同程序间通信）

### 1.3 为什么需要进程池？

- 一次性开启指定数量的进程
- 如果有十个进程，有一百个任务，一次可以处理多少个（一次性只能处理十个）
- 防止进程开启数量过多导致服务器压力过大
- 进程池中有两个方法：
  - `1）apply`： 多个进程异步执行，一个一个的执行
  - `2）apply_async`： 多个进程同步执行，同时执行多个进程

```python
from  multiprocessing import Process,Pool
import time,os
def foo(i):
    time.sleep(2)
    print("in the process",os.getpid()) #打印子进程的pid
    return i+100

def call(arg):
    print('-->exec done:',arg,os.getpid())

if __name__ == '__main__':
    pool = Pool(3)                      #进程池最多允许5个进程放入进程池
    print("主进程pid：",os.getpid())     #打印父进程的pid
    for i in range(10):
        #用法1 callback作用是指定只有当Foo运行结束后就执行callback调用的函数,父进程调用的callback函数
        pool.apply_async(func=foo, args=(i,),callback=call)

        #用法2 串行 启动进程不在用Process而是直接用pool.apply()
        # pool.apply(func=foo, args=(i,))

    print('end')
    pool.close()    #关闭pool
    pool.join()     #进程池中进程执行完毕后再关闭，如果注释，那么程序直接关闭。
```



### 1.4 僵尸进程、孤儿进程、守护进程

- ```
  1）僵尸进程定义
  ```

  - 僵尸进程产生的原因就是`父进程产生子进程后，子进程先于父进程退出, 而父进程没有回收子进程的资源`
  - 一个进程使用fork创建子进程，如果子进程退出，而父进程并没有调用wait或waitpid获取子进程的状态信息
  - 那么子进程的进程描述符仍然保存在系统中。这种进程称之为僵死进程。
- 简而言之，子进程退出后，父进程没有回收子进程的资源就退出
  
- ```
  2）孤儿进程
  ```

  - 父进程异常结束了，然后子进程被1号进程init收养

- ```
  3）守护进程
  ```

  - 是创建守护进程时有意把父进程结束，然后被1号进程init收养

- `4）用python写一个僵尸进程`

```python
#!/usr/bin/env python
#coding=utf8
 
import os, sys, time
#产生子进程
pid = os.fork()
 
if pid == 0:
    #子进程退出
    sys.exit(0)
#父进程休息30秒
time.sleep(30)
# 先产生一个子进程，子进程退出，父进程休息30秒,那就会产生一个僵尸进程
```

- ps -ef| grep defunct` 在linux下查看僵尸进程

  - ```sh
    [root@linux-node4 ~]# ps -ef| grep defunct
    root     110401  96083  0 19:11 pts/2    00:00:00 python defunct.py
    root     110402 110401  0 19:11 pts/2    00:00:00 [python] <defunct>
    root     110406  96105  0 19:11 pts/3    00:00:00 grep --color=auto defunct
    ```




### 1.5 Python中使用过的进程模块？

#### 1.5.1 multiprocessing

- `multiprocessing`是一个使用类似于线程模块的API支持产生进程的包。
- 多处理包提供本地和远程并发，通过使用子进程而不是线程有效地侧向执行全局解释器锁。
- 因此，多处理模块允许程序员充分利用给定机器上的多个处理器。 它可以在Unix和Windows上运行。
- `进程池抓取页面`

```python
# -*- coding: utf-8 -*-
import requests
from multiprocessing import Pool

def fetch_request(url):
    result = requests.get(url)
    print(result.text)

def call(arg):
    print('-->exec done:',"测试进程池执行后回调功能")

url_list = [
    'https://www.baidu.com',
    'https://www.google.com/',         #google页面会卡住，知道页面超时后这个进程才结束
    'http://dig.chouti.com/',          #chouti页面内容会直接返回，不会等待Google页面的返回
]

if __name__ == '__main__':
    pool = Pool(10)        # 创建线程池
    for url in url_list:
        #用法1 callback作用是指定只有当Foo运行结束后就执行callback调用的函数,父进程调用的callback函数
        pool.apply_async(func=fetch_request, args=(url,),callback=call)
    print('end')
    pool.close()    #关闭pool
    pool.join()     #进程池中进程执行完毕后再关闭，如果注释，那么程序直接关闭。
```



#### 1.5.2 concurrent.futures

- `1、简介` [参考官网(opens new window)](https://docs.python.org/3/library/concurrent.futures.html)
  - 1、Python标准库为我们提供了threading和multiprocessing模块编写相应的多线程/多进程代码
  - 2、但是当项目达到一定的规模，频繁创建/销毁进程或者线程是非常消耗资源的，这个时候我们就要编写自己的线程池/进程池，以空间换时间。
  - 3、但从Python3.2开始，标准库为我们提供了concurrent.futures模块，它提供了ThreadPoolExecutor和ProcessPoolExecutor两个类，
  - 4、实现了对threading和multiprocessing的进一步抽象，对编写线程池/进程池提供了直接的支持。
- `2、Executor和Future`
  - `1. Executor`
  - concurrent.futures模块的基础是Exectuor，Executor是一个抽象类，它不能被直接使用。
  - 但是它提供的两个子类ThreadPoolExecutor和ProcessPoolExecutor却是非常有用
  - 我们可以将相应的tasks直接放入线程池/进程池，不需要维护Queue来操心死锁的问题，线程池/进程池会自动帮我们调度。
  - `2. Future`
  - Future你可以把它理解为一个在未来完成的操作，这是异步编程的基础，
  - 传统编程模式下比如我们操作queue.get的时候，在等待返回结果之前会产生阻塞，cpu不能让出来做其他事情
  - 而Future的引入帮助我们在等待的这段时间可以完成其他的操作。
- 3、concurrent.futures.ProcessPoolExecutor 抓取网页

```python
import requests
from concurrent.futures import ProcessPoolExecutor

def fetch_request(url):
    result = requests.get(url)
    print(result.text)

url_list = [
    'https://www.baidu.com',
    'https://www.google.com/',         #google页面会卡住，知道页面超时后这个进程才结束
    'http://dig.chouti.com/',          #chouti页面内容会直接返回，不会等待Google页面的返回
]

if __name__ == '__main__':
    pool = ProcessPoolExecutor(10)        # 创建线程池
    for url in url_list:
        pool.submit(fetch_request,url)    # 去线程池中获取一个进程，进程去执行fetch_request方法
    pool.shutdown(wait = True)
    # shutdown相当于一个开关，它会读取程序中所设定
```



# 线程

## 2.1 什么是线程

- 线程是操作系统调度的最小单位

- 线程是进程真正的执行者， 是一些指令的合集（是进程资源的拥有者）

- 同一进程下的多个线程共享内存空间， 数据可以直接访问（数据共享）

- 为了保证线程安全，会使用**线程锁**

- ```python
  import threading
  import time
  
  def sayhi(num): #定义每个线程要运行的函数
      print("running on number:%s" %num)
      time.sleep(3)
  for i in range(50):
      t = threading.Thread(target=sayhi,args=('t-%s'%i,))
      t.start()
  
  ```



## 2.2 GiL锁和线程锁

-  Gil全局解释器锁
  - 保证同一时间只有一个线程在运行
  - 防止多个线程都修改数据
  - 实质
    - 每个线程在执行的时候都需要先获取Gil, 保证同一时刻只有一个线程执行可以执行代码
    - 即同一时刻只有一个线程使用cpu
- 线程锁（互斥锁）
  - GIL 锁只能保证同一时刻只有一个线程对某个资源操作，但是由于设计之初考虑到死锁的问题，会有一个chek-interval 机制，会在一段时候后自动释放锁，所以会导致可能上一个线程还为执行完毕就释放锁
  - 线程锁的本质是将线程中的数据加了一把互斥锁
    - 加了线程锁之后，其他线程无法访问该数据，包括读
  - 有了GIL全局解释器锁，为什么还需要线程锁
    - Gil是限制同一时间只有一个线程进入python 解释器的
    - 由于cpu是分时轮询调度的，导致多线程有可能会造成数据的安全问题
    - 还有一个原因是GIL自带的check_interval机制，会强制释放锁，也可能会导致数据安全问题
- 为什么python 中需要全局解释器锁
  - cpython的实现中，Gil是为了解决有引用计数问题带来的资源竞争漏洞问题的
  - cpython中的引用计数是全局，局部变量和对象字段等都参与到了引用计数中
  - 当一个线程运行时，会先去获得Gil解释器锁，保证引用计数时全局更新的
  - 如果没有Gil，  当两个线程同时对一个对象进行引用时，引用计数只会提高1，而不是2

## 2.3 join



- 实现所有线程都结束之后，在执行主线程

- ```python
  import threading
  import time
  start_time = time.time()
  
  def sayhi(num): #定义每个线程要运行的函数
      print("running on number:%s" %num)
      time.sleep(30)
  
  t_objs = []    #将进程实例对象存储在这个列表中
  for i in range(50):
      t = threading.Thread(target=sayhi,args=('t-%s'%i,))
      t.start()          #启动一个线程，程序不会阻塞
      t_objs.append(t)
  print(threading.active_count())    #打印当前活跃进程数量
  for t in t_objs: #利用for循环等待上面50个进程全部结束
      t.join()     #阻塞某个程序
  print(threading.current_thread())    #打印执行这个命令进程
  
  print("----------------all threads has finished.....")
  print(threading.active_count())
  print('cost time:',time.time() - start_time)
  ```

## 2.4 setDaemon

- 守护线程，主线程退出时，需要子线程随主线程同时退出

```python
import threading
import time
start_time = time.time()

def sayhi(num): #定义每个线程要运行的函数
    print("running on number:%s" %num)
    time.sleep(3)
for i in range(50):
    t = threading.Thread(target=sayhi,args=('t-%s'%i,))
    t.setDaemon(True)  #把当前线程变成守护线程，必须在t.start()前设置
    t.start()          #启动一个线程，程序不会阻塞
print('cost time:',time.time() - start_time)
```



## 2.5 python中使用过哪些线程模块

- Python提供了几个用于多线程编程的模块，包括thread、threading和Queue等。

- thread和threading模块允许程序员创建和管理线程。

- ```python
  import threading
  import time
  
  def sayhi(num): #定义每个线程要运行的函数
      print("running on number:%s" %num)
      time.sleep(3)
      
  for i in range(50):
      t = threading.Thread(target=sayhi,args=('t-%s'%i,))
      t.start()
  ```

- 

## 2.6 死锁

- 两个以上的进程或线程在执行过程中，因争夺资源而造成的一种互相等待的现象
- 若无外力作用，它们都将无法推进下去。

2.7 信号量（Semaphore）

- 1.互斥锁 同时只允许一个线程更改数据，而Semaphore是同时允许一定数量的线程更改数据
- 2.比如厕所有3个坑，那最多只允许3个人上厕所，后面的人只能等里面有人出来了才能再进去
- 3.作用就是同一时刻允许运行的线程数量

```python
import threading,time

def run(n):
    semaphore.acquire()
    time.sleep(1)
    print("run the thread: %s\n" %n)
    semaphore.release()

if __name__ == '__main__':
    semaphore = threading.BoundedSemaphore(5)  #最多允许5个线程同时运行
    for i in range(22):
        t = threading.Thread(target=run,args=(i,))
        t.start()

while threading.active_count() != 1:
    pass  # print threading.active_count()
else:
    print('----all threads done---')

# 代码结果说明：这里可以清晰看到运行时0-4是同时运行的没有顺序，而且是前五个，
# 表示再semaphore这个信号量的定义下程序同时仅能执行5个线程
```



# 协程

## 3.1 什么时协程

- 协程时微线程，本质时一个单线程
- 协程能在单线程处理高并发，因为遇到io会自动切换
  - 线程遇到io会等待，阻塞，协程遇到io会自动切换
  - 线程的状态保存在cpu的寄存器和栈里，而协程拥有自己的空间，无序上下问切换，所以快
- 为什么协程遇到io可以自动切换
  - greenlet是C语言写的一个模块，遇到IO手动切换
  -  协程有一个gevent模块（分装了greelet模块），遇到io能自动切换
  -  协程拥有自己的空间，所以无需上下文切换的开销 

## 3.2 协程的优缺点

- 缺点
  - 无法利用多核的资源：协程本质就是一个单线程，他不能同时将单个cpu的多核利用起来，协程需要和进程配合，才能利用多核 cpu
  - 协程如果阻塞，整个程序都会阻塞
- 优点
  - 不仅能处理高并发（单线程下的高并发）
  - 而且特别节省资源（本质是一个单线程，当然节省资源）
- 协程遇到io自动切换，那谁来干活
  - 简单说法
    - 协程遇到I/O后自动切换，但是会保持一个socket连接，交给系统内核去处理工作
    - epoll()就工作内核中，他维护了一个链表，来存放所有的socket连接
    - 当内核处理完成后就会回调一个函数，以socket文件描述符为key，结果为value存放到字典中
    - 此时这个列表还是在内核中，需要将这个字典拷贝到用户空间（用户进程中）
  - 本质
    - 1.`epoll()中内核则维护一个链表`，epoll_wait直接检查链表是不是空就知道是否有文件描述符准备好了。
    - 2.在内核实现中epoll是根据每个sockfd上面的与设备驱动程序建立起来的回调函数实现的。
    - 3.某个sockfd上的事件发生时，与它对应的回调函数就会被调用，来把这个sockfd加入链表，其他处于“空闲的”状态的则不会。
    - 4.epoll上面链表中获取文件描述，这里使用内存映射（mmap）技术，避免了复制大量文件描述符带来的开销
    - 内存映射（mmap）：内存映射文件，是由一个文件到一块内存的映射，将不必再对文件执行I/O操作

## 3.4 Python中协程的模块

- greenlet：遇到I/O`手动切换`，是一个C模块
- gevent：对greenlet封装，遇到I/O`自动切换`（`借助C语言库greenlet`）
- asyncio：和gevent一样，也是实现协程的一个模块（`python自己实现`）





## 进程池、线程池、协程池对比

- 进程池：浪费资源

```python
# -*- coding: utf-8 -*-
import requests
from multiprocessing import Pool

def fetch_request(url):
    result = requests.get(url)
    print(result.text)

def call(arg):
    print('-->exec done:',"测试进程池执行后回调功能")

url_list = [
    'https://www.baidu.com',
    'https://www.google.com/',         #google页面会卡住，知道页面超时后这个进程才结束
    'http://dig.chouti.com/',          #chouti页面内容会直接返回，不会等待Google页面的返回
]

if __name__ == '__main__':
    pool = Pool(10)        # 创建线程池
    for url in url_list:
        # 用法1 callback作用是指定只有当Foo运行结束后就执行callback调用的函数,父进程调用的callback函数
        pool.apply_async(func=fetch_request, args=(url,),callback=call)
    print('end')
    pool.close()    #关闭pool
    pool.join()     #进程池中进程执行完毕后再关闭，如果注释，那么程序直接关闭。
```

- 线程池
  -  创建一个新线程将消耗大量的计算资源，并且在阻塞过程中无法执行其他任务。 
  -  比如线程池中10个线程同时去10个url获取数据，当数据还没来时这些线程全部都在等待，不做事。 

```python
import requests
from concurrent.futures import ThreadPoolExecutor

def fetch_request(url):
    result = requests.get(url)
    print(result.text)

url_list = [
    'https://www.baidu.com',
    'https://www.google.com/',         #google页面会卡住，知道页面超时后这个进程才结束
    'http://dig.chouti.com/',          #chouti页面内容会直接返回，不会等待Google页面的返回
]

pool = ThreadPoolExecutor(10)            # 创建一个线程池，最多开10个线程
for url in url_list:
    pool.submit(fetch_request,url)       # 去线程池中获取一个线程，线程去执行fetch_request方法

pool.shutdown(True)                      # 主线程自己关闭，让子线程自己拿任务执行
```

- 协程
  - 特点： gevent只用起一个线程，当请求发出去后gevent就不管,永远就只有一个线程工作，谁先回来先处理 

```python
import gevent
from gevent import monkey
monkey.patch_all(select=False)  # 注意，这个导包顺序不要变
import requests

# 这些请求谁先回来就先处理谁
def fetch_async(method, url, req_kwargs):
    response = requests.request(method=method, url=url, **req_kwargs)
    print(response.url, response.content)

# ##### 发送请求 #####
gevent.joinall([
    gevent.spawn(fetch_async, method='get', url='https://www.baidu.com/', req_kwargs={}),
    gevent.spawn(fetch_async, method='get', url='https://www.google.com/', req_kwargs={}),
    gevent.spawn(fetch_async, method='get', url='https://github.com/', req_kwargs={}),
])
```

